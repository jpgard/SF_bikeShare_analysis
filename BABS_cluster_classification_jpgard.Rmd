---
title: "STATS 415 Final Project"
author: "Joshua Gardner"
date: "April 15, 2016"
output: html_document
---

#Introduction

The questions of interest for this analysis are: 

*Can we find distinct clusters of riders, based on the features available in the Bay Area Bike Share open dataset?
*Can we predict ride length (short, medium, or long) using k-nearest neighbors classification?

#The Data

This analysis uses three different datasets provided as part of the Bay Area Bike Share (BABS) open dataset. Files contain data from  3/1/14 to 8/31/14.

####Station Information:

FILE = "201408_station_data.csv"


*station_id: station ID number (corresponds to "station_id" in "201408_status_data.csv")
*name: name of station
*lat: latitude
*long: longitude
*dockcount: number of total docks at station
*landmark: city (San Francisco, Redwood City, Palo Alto, Mountain View, San Jose)
*installation: date that station was installed 

####Trip Data:

FILE = "201408_trip_data.csv"


-Trip ID: numeric ID of bike trip
-Duration: time of trip in seconds
-Start Date: start date of trip with date and time, in PST
-Start Station: station name of start station
-Start Terminal: numeric reference for start station
-End Date: end date of trip with date and time, in PST
-End Station: station name for end station
-End Terminal: numeric reference for end station
-Bike #: ID of bike used
-Subscription Type: Subscriber = annual member; Customer = 24-hour or 3-day member
-Zip Code: Home zip code of user (only available for annual members)


####Weather Data:

FILE = "201408_weather_data.csv"
Daily weather information per service area. Weather is listed from north to south (San Francisco, Redwood City, Palo Alto, Mountain View, San Jose).
    
-Max_Visibility_Miles 	
-Mean_Visibility_Miles 	
-Min_Visibility_Miles 	 		
-Precipitation_In 	"numeric, in form x.xx but alpha ""T""= trace when amount less than .01 inch"	
-Cloud_Cover 	"scale of 0-8, 0=clear"	
-Events	"text field - entries: rain, fog, thunderstorm"	
-zip code: 94107=San Francisco, 94063=Redwood City, 94301=Palo Alto, 94041=Mountain View, 95113= San Jose"

#Analysis and Methods


```{r}
library(plyr)
library(dplyr)
library(tidyr)
library(ggmap)
library(qdapRegex)
library(geosphere)


stationdata = read.csv("babs_open_data_year_2/201508_station_data.csv")
tripdata = read.csv("babs_open_data_year_2/201508_trip_data.csv")
weatherdata = read.csv("babs_open_data_year_2/201508_weather_data.csv")

weather_subset = weatherdata[,c("PDT", "Max.TemperatureF", "Mean.TemperatureF", "Min.TemperatureF", "Mean.Humidity", "Mean.VisibilityMiles", "Mean.Wind.SpeedMPH", "Max.Gust.SpeedMPH", "PrecipitationIn", "CloudCover", "Events", "Zip")]

tripdata <- tripdata %>%
    mutate(StartDateTime = as.POSIXct(Start.Date, format = "%m/%d/%Y %H:%M", tz = "Pacific")) %>%
    mutate(EndDateTime = as.POSIXct(End.Date, format = "%m/%d/%Y %H:%M", tz = "Pacific")) %>%
    separate(Start.Date, into = c("StartDate", "StartTime"), sep = " ") %>%
    separate(End.Date, into = c("EndDate", "EndTime"), sep = " ")

#reverse geocode; commented out because data was saved to file

#results = data.frame()
#for (i in 1:nrow(stationdata)) {
#    address = revgeocode(c(stationdata[i,c("long")], stationdata[i,c("lat")]))
#    results = rbind(results, data.frame("name" = stationdata[i,c("name")], "address" = address))
#}

#saves results of station addresses to file
#write.csv(results, file = "station_addresses.csv")

#reads saved results data from file
results = read.csv("station_addresses.csv")

#add address to stationdata, extract ZIP codes to new variable
stationdata <- stationdata %>%
    merge(results) %>%
    mutate(ZIP = ex_zip(text.var = address))

#use station name to merge stationdata with tripdata, add only ZIPs to tripdata
#use ZIP and date to merge with weather data for each trip
#create variables for start_lat, start_long, end_lat, end_long using stationdata
#remove 'T' for trace precipitation; replace w/ 0 so this variable is continuous and can be used in PCA
#omit na
#filter to remove 3 rides that were > 50000 seconds (multiple days long); skews plots and analysis

tripdata_final <- tripdata %>%
    na.omit() %>%
    merge(stationdata[,c("name", "ZIP", "lat", "long")], by.x = "Start.Station", by.y = "name") %>%
    merge(weather_subset, by.x = c("StartDate", "ZIP"), by.y = c("PDT", "Zip")) %>%
    transform(start_lat = lat, start_long = long) %>%
    subset(select = -c(lat, long)) %>%
    merge(stationdata[,c("name", "lat", "long")], by.x = "End.Station", by.y = "name") %>%
    transform(end_lat = lat, end_long = long) %>%
    subset(select = -c(lat, long)) %>%
    mutate(PrecipitationIn = revalue(PrecipitationIn, c("T" = 0))) %>%
    na.omit() %>%
    filter(Duration <= 50000)
    
#row-wise computation of Great Circle distance between start and end locations for each trip; use this as a proxy for trip distance    
for (i in 1:nrow(tripdata_final)) {
    start_long = tripdata_final[i,c("start_long")]
    start_lat = tripdata_final[i,c("start_lat")]
    end_long = tripdata_final[i,c("end_long")]
    end_lat = tripdata_final[i,c("end_lat")]
    tripdata_final$distGeo[i] = distGeo(c(start_long, start_lat), c(end_long, end_lat))
}

for (i in 1:nrow(tripdata_final)) {
    start_hr = strsplit(tripdata_final[i,c("StartTime")], split = ":")[[1]][1]
    start_min = strsplit(tripdata_final[i,c("StartTime")], split = ":")[[1]][2]
    tripdata_final$StartTimeMin[i] = as.numeric(start_hr)*60+as.numeric(start_min)
}
    
rownames(tripdata_final) <- tripdata_final$Trip.ID    


```


###Exploratory and Summary Plots
```{r}
library(ggplot2)

#jitter plot or boxplot of trip durations

#geo heatmap of starting locations and ending locations

#faceted plot showing distribution of mean temperature, mean humidity, mean visibility, mean wind speed to show variability of weather data

#histogram of trip start & end times

```


#Principal Components Analysis and Visualization

##Introduction

Here, we apply Principal Component Analysis (PCA) to the data to explore the possibility of using dimension reduction to reduce the number of predictors used for the analysis and modeling tasks below, and as an unsupervised exploratory tool to measure the relationships between different variables in the dataset.

##Implementation

Principal Component Analysis was implemented using the subset of data produced by the preprocessing above, and included all of the continuous variables in this dataset (both variables that were originally provided, and those that were derived from the original data, such as distGeo). PCA was performed on the scaled data with mean zero and standard deviation one, because data were measured in several different units and displayed large differences in variances. The **prcomp()** function in R was used to generate these principal components, but any other PCA function would generate the same principal components, as each principal component loading vector is unique to the dataset (up to a sign flip, because flipping the sign of the vector has no effect on its position in p-dimentional space; each principal component vector extends in both directions).

```{r}
#subset only continuous variables, convert columns to numeric as necessary

tripdata_pca <- tripdata_final %>%
    subset(select = c(Duration, distGeo, Max.TemperatureF, Mean.TemperatureF, Min.TemperatureF, Mean.Humidity, Mean.VisibilityMiles, Mean.Wind.SpeedMPH, Max.Gust.SpeedMPH, PrecipitationIn, CloudCover, StartTimeMin)) %>%
    mutate(PrecipitationIn = as.numeric(as.character(PrecipitationIn))) 

pr.out = prcomp(tripdata_pca, scale = TRUE)
```

##Data Analysis: PCA Results
```{r}
pr.out
```

```{r}
#compute variance explained by each principal component, and then share of variance explained by each
pr.var=pr.out$sdev^2
pve=pr.var/sum(pr.var)

biplot(pr.out, scale = 0, xlabs=rep(".", nrow(tripdata_pca)))
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b', main = "Fig. 1: Proportion of Variance Explained By Principal Components\nof BABS Trip and Weather Data")
plot(cumsum(pve), xlab="Principal Component ", ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b', main = "Fig. 2: Cumulative Proportion of Variance Explained By Principal Components\nof BABS Trip and Weather Data")
```

The loading vector for each of the principal components are shown above. We see that there are 12 distinct principal components, which we would expect because there are in general min(n âˆ’ 1, p) informative principal components in a data set with n observations and p variables. Principal components were calculated using scaled data, because several different units were used to measure the different predictors and the predictors had large differences in their variance, due in part to these differences in measurement units. We also calculated the variance explained by each principal component, obtained by squaring the standard deviation of each principal component, and computed the cumulative proportion of the variance explained by each principal component by dividing the variance explained by each individual principal component by the sum of the variance of all principal components. Then, we utilized the cumsum() function in R to compute the cumulative sums for the plot above.

As Fig. 1, also known as a scree plot, shows, there is a relatively smooth decline in the proportion of the variance explained by each successive principal component. Ideally, we would hope for an "elbow" shape in the scree plot, where the proportion of variance explained decreases sharply and then begins to level off, but this isn't the case here. Principal Components Analysis is intended to be a dimensionality reduction technique--that is, to reduce the number of variables needed to describe the relationship between data points--but, for the data here with the inputs used, this isn't a particularly effective approach, or at least there isn't a clear cutoff as to which principal component vectors we should include.

We can also examine the loading vectors individually, however, and examine which variables achieved high factor loadings. Variables that are loading on the same factor should usually "make sense" together, indicating that this factor is actually picking up and reducing a real multivariate relationship into a univariate one (in the case of a single factor). Indeed, we see that the loading vector for the first principal component, listed as PC1 above, has high loadings for *Max.TemperatureF*, *Mean.TemperatureF*, *Min.TemperatureF*, suggesting that it is a reduction of variables related to temperature. This seems certainly like a valuable principal component, as we can intuitively understand why whether would affect the use of bike-sharing programs: users who do not own bikes, but merely rent them from BABS on occasion, would be expected to take that day's weather, and especially its temperature, into account when deciding whether or not to rent a bike (which is inherently an outdoor activity) on a given day. 

Additionally, in the second principal component loading vector, we can see high loadings for *Mean.Wind.SpeedMPH*, *Mean.Wind.SpeedMPH*, and *CloudCover*, suggesting that this second vector is picking up conditions related to wind and weather patterns--again, factors that seem to be closely connected to each other (and thus reasonable candidates for dimensionality reduction in a principal components vector).

We also noted the high loadings in PC4, the loading vector for principal component four, for *Duration*, *distGeo* (geographic distance between the start and end stations), and *StartTimeMin* (the numeric minute of the day when the ride started), suggesting that this principal component is picking up on features of the ride itself (distance, duration, and time of day the ride began at)--again, elements that are clearly related to one another and would again be reasonable candidates for dimensionality reduction via principal component analysis.

##Implications for Parameter Selection

Principal components analysis conducted on the August 2015 Bay Area Bike Share data results in meaningful principal components, but these principal components do not explain a substantial enough share of the variance to suggest that these would be a useful solution to many prediction or modeling problems with this dataset. As Hastie and Tibshirani write in Introduction to Statistical Learning, "In practice, we tend to look at the first few principal components in order to find interesting patterns in the data. If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest" (384). Such is the case here--while this is a valuable exploratory data analysis exercise, the principal components are unlikely to be of any practical use in this analysis because they do not explain enough of the variance to be practically useful.

The relatively low effectiveness of the principal component analysis approach is likely due, at least in part, to the fact that many meaningful variables in the BABS dataset are factor variables, not continuous variables, and therefore not condidates for use in PCA. Using a dimension reduction approach that could handle these types of variables, such as multiple factor analysis (MFA) might be a reasonable next step, but is beyond the scope of this project.


#Clustering and Visualization

##Introduction

Clustering, which is an unsupervised technique used to find similar groups in a dataset, is another useful exploratory technique that might be a better way to explore similarities between types of rides in our data. The general task for clustering is to partition the data into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. Both clustering and PCA are ways of exploring trends and patterns in the data and finding a small number of summary statistics, but clustering specifically looks to create subgroups, while PCA simply looks to find smaller numbers of variables (individually and in combination) that explain a maximum amount of the variance in the data.

Clustering, in particular, might be a real task the administrators of the Bay Area Bike Share would be interested: having a clear understanding of the different types of rides their customers are most likely to take would allow BABS to meet customer needs by allocating bikes correctly, and to market their service effectively by appealing to the types of riders most likely to use the service.

##Implementation

We elected to implement heirarchical clustering to explore our dataset instead of using an alternative clustering method like K-means clustering. While k-means clustering can be a useful method, it requires specifying the number of clusters (or experimenting with several values of k); we found heirarchical clustering to be a more useful way to explore the data with fewer assumptions and restrictions as to how it would be clustered. We used **average linkage** and **complete linkage** as the dissimilarity measures for our initial clustering, because these two methods are generally considered to yield more balanced clusters.

We again elected to scale the data prior to clustering, due to the different units used to measure the variables being used for the clustering model (which included trip information--distance, duration, and start time--as well as several types of weather information).

Cluster validation was performed primarily through visualization--however, if we intended to have any measure of our confidence in the clusters, their p-values, etc., a more rigorous statistical approach would need to be applied. Such a validation approach is beyond the scope of this project (and this course), and there appears to be little consensus on effective cluster validation among the statistical learning community at present.

####Note: try both correlation-based and euclidean distance

##Data Analysis: Clustering Results

##Implications for Parameter Selection






