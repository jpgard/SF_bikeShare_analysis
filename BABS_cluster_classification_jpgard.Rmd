---
title: "STATS 415 Final Project"
author: "Joshua Gardner"
date: "April 15, 2016"
output: html_document
---

#Introduction

The questions of interest for this analysis are: 

*Can we find distinct clusters of riders, based on the features available in the Bay Area Bike Share open dataset?
*Can we predict ride length (short, medium, or long) using k-nearest neighbors classification?

#The Data

This analysis uses three different datasets provided as part of the Bay Area Bike Share (BABS) open dataset. Files contain data from  3/1/14 to 8/31/14.

####Station Information:

FILE = "201408_station_data.csv"


*station_id: station ID number (corresponds to "station_id" in "201408_status_data.csv")
*name: name of station
*lat: latitude
*long: longitude
*dockcount: number of total docks at station
*landmark: city (San Francisco, Redwood City, Palo Alto, Mountain View, San Jose)
*installation: date that station was installed 

####Trip Data:

FILE = "201408_trip_data.csv"


-Trip ID: numeric ID of bike trip
-Duration: time of trip in seconds
-Start Date: start date of trip with date and time, in PST
-Start Station: station name of start station
-Start Terminal: numeric reference for start station
-End Date: end date of trip with date and time, in PST
-End Station: station name for end station
-End Terminal: numeric reference for end station
-Bike #: ID of bike used
-Subscription Type: Subscriber = annual member; Customer = 24-hour or 3-day member
-Zip Code: Home zip code of user (only available for annual members)


####Weather Data:

FILE = "201408_weather_data.csv"
Daily weather information per service area. Weather is listed from north to south (San Francisco, Redwood City, Palo Alto, Mountain View, San Jose).
    
-Max_Visibility_Miles 	
-Mean_Visibility_Miles 	
-Min_Visibility_Miles 	 		
-Precipitation_In 	"numeric, in form x.xx but alpha ""T""= trace when amount less than .01 inch"	
-Cloud_Cover 	"scale of 0-8, 0=clear"	
-Events	"text field - entries: rain, fog, thunderstorm"	
-zip code: 94107=San Francisco, 94063=Redwood City, 94301=Palo Alto, 94041=Mountain View, 95113= San Jose"

#Analysis and Methods


```{r}
library(plyr)
library(dplyr)
library(tidyr)
library(ggmap)
library(qdapRegex)
library(geosphere)


stationdata = read.csv("babs_open_data_year_2/201508_station_data.csv")
tripdata = read.csv("babs_open_data_year_2/201508_trip_data.csv")
weatherdata = read.csv("babs_open_data_year_2/201508_weather_data.csv")

weather_subset = weatherdata[,c("PDT", "Max.TemperatureF", "Mean.TemperatureF", "Min.TemperatureF", "Mean.Humidity", "Mean.VisibilityMiles", "Mean.Wind.SpeedMPH", "Max.Gust.SpeedMPH", "PrecipitationIn", "CloudCover", "Events", "Zip")]

tripdata <- tripdata %>%
    mutate(StartDateTime = as.POSIXct(Start.Date, format = "%m/%d/%Y %H:%M", tz = "Pacific")) %>%
    mutate(EndDateTime = as.POSIXct(End.Date, format = "%m/%d/%Y %H:%M", tz = "Pacific")) %>%
    separate(Start.Date, into = c("StartDate", "StartTime"), sep = " ") %>%
    separate(End.Date, into = c("EndDate", "EndTime"), sep = " ") %>%
    rename(c("Bike.." = "Bike_number"))

results = data.frame()
for (i in 1:nrow(stationdata)) {
    address = revgeocode(c(stationdata[i,c("long")], stationdata[i,c("lat")]))
    results = rbind(results, data.frame("name" = stationdata[i,c("name")], "address" = address))
}

#saves results of station addresses to file
#write.csv(results, file = "station_addresses.csv")

#add address to stationdata, extract ZIP codes to new variable
stationdata <- stationdata %>%
    merge(results) %>%
    mutate(ZIP = ex_zip(text.var = address))

#use station name to merge stationdata with tripdata, add only ZIPs to tripdata
#use ZIP and date to merge with weather data for each trip
#create variables for start_lat, start_long, end_lat, end_long using stationdata
#remove 'T' for trace precipitation; replace w/ 0 so this variable is continuous and can be used in PCA
#omit na
#filter to remove 3 rides that were > 50000 seconds (multiple days long); skews plots and analysis

tripdata_final <- tripdata %>%
    na.omit() %>%
    merge(stationdata[,c("name", "ZIP", "lat", "long")], by.x = "Start.Station", by.y = "name") %>%
    merge(weather_subset, by.x = c("StartDate", "ZIP"), by.y = c("PDT", "Zip")) %>%
    transform(start_lat = lat, start_long = long) %>%
    subset(select = -c(lat, long)) %>%
    merge(stationdata[,c("name", "lat", "long")], by.x = "End.Station", by.y = "name") %>%
    transform(end_lat = lat, end_long = long) %>%
    subset(select = -c(lat, long)) %>%
    mutate(PrecipitationIn = revalue(PrecipitationIn, c("T" = 0))) %>%
    na.omit() %>%
    filter(Duration <= 50000)
    
#row-wise computation of Great Circle distance between start and end locations for each trip; use this as a proxy for trip distance    
for (i in 1:nrow(tripdata_final)) {
    start_long = tripdata_final[i,c("start_long")]
    start_lat = tripdata_final[i,c("start_lat")]
    end_long = tripdata_final[i,c("end_long")]
    end_lat = tripdata_final[i,c("end_lat")]
    tripdata_final$distGeo[i] = distGeo(c(start_long, start_lat), c(end_long, end_lat))
}

for (i in 1:nrow(tripdata_final)) {
    start_hr = strsplit(tripdata_final[i,c("StartTime")], split = ":")[[1]][1]
    start_min = strsplit(tripdata_final[i,c("StartTime")], split = ":")[[1]][2]
    tripdata_final$StartTimeMin[i] = as.numeric(start_hr)*60+as.numeric(start_min)
}
    
rownames(tripdata_final) <- tripdata_final$Trip.ID    


```


###Exploratory and Summary Plots
```{r}
library(ggplot2)

#jitter plot or boxplot of trip durations

#geo heatmap of starting locations and ending locations

#faceted plot showing distribution of mean temperature, mean humidity, mean visibility, mean wind speed to show variability of weather data

#histogram of trip start & end times

```


#Principal Components Analysis and Visualization

```{r}
#subset only continuous variables, convert columns to numeric as necessary

tripdata_pca <- tripdata_final %>%
    subset(select = c(Duration, distGeo, Max.TemperatureF, Mean.TemperatureF, Min.TemperatureF, Mean.Humidity, Mean.VisibilityMiles, Mean.Wind.SpeedMPH, Max.Gust.SpeedMPH, PrecipitationIn, CloudCover, StartTimeMin)) %>%
    mutate(PrecipitationIn = as.numeric(as.character(PrecipitationIn))) 

pr.out = prcomp(tripdata_pca, scale = TRUE)

#compute variance explained by each principal component, and then share of variance explained by each
pr.var=pr.out$sdev^2
pve=pr.var/sum(pr.var)

biplot(pr.out, scale = 0)
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component ", ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')


```

#Clustering and Visualization







